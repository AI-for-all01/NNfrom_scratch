{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rnd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Model():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        # Layers will store the informations we need to create the layers\n",
    "        # Layers will be created only when the feedforward begins because we need to know\n",
    "        # the shape of the input layer, which is store as None for now\n",
    "        self.layers_info = []\n",
    "        self.n_layers = 0\n",
    "        #self.layers_initiated = False\n",
    "\n",
    "    def add_layer(self, size, activation):\n",
    "        self.layers_info.append({\"size\": size,\n",
    "                \"activation\": activation,\n",
    "                \"index\": self.n_layers})\n",
    "        self.n_layers += 1\n",
    "\n",
    "    def initiate_layers(self, input_shape):\n",
    "        # Create all the weights and biases of the model\n",
    "        # Takes the input_shape in parameter because it needs to know how many input neurons there is to create the\n",
    "        # appropriat number of weights and biases\n",
    "\n",
    "        # Initiate the value of prev_layer_size to the size of the input layer\n",
    "        prev_layer_size = 1\n",
    "        for d in input_shape:\n",
    "            prev_layer_size *= d\n",
    "\n",
    "        # Creates all the other layer\n",
    "        for l in range(len(self.layers_info)):\n",
    "            # Takes the number of neurons in the l-1 th and in the lth layer to initiate the weights and biases in the\n",
    "            # l th layer\n",
    "            current_layer_size = self.layers_info[l][\"size\"]\n",
    "            shape = [current_layer_size, prev_layer_size]\n",
    "            self._create_layer(shape)\n",
    "\n",
    "            # Change the value of prev_layer_size for the next iteration\n",
    "            prev_layer_size = current_layer_size\n",
    "\n",
    "    def predict(self, output, target):\n",
    "        \"\"\"prediction = np.argmax(outputs)\n",
    "        if target[prediction] == 1:\n",
    "            return True\n",
    "        else:\n",
    "            return False\"\"\"\n",
    "\n",
    "        if output > 0.5:\n",
    "            prediction = 1\n",
    "        else:\n",
    "            prediction = 0\n",
    "\n",
    "        if prediction == target[0]:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "    def predict_highest(self, output, target):\n",
    "        prediction = np.argmax(outputs)\n",
    "        if target[prediction] == 1:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, inputs, targets, epochs, eta=0.01, mini_batches_size=10):\n",
    "        training_set = self.create_training_set(inputs, targets)\n",
    "        for i in range(epochs):\n",
    "            #tracking\n",
    "            cost = 0\n",
    "            accuracy = 0\n",
    "            #tracking\n",
    "\n",
    "            n_mini_batches = int( len(training_set)/mini_batches_size )\n",
    "            print(n_mini_batches)\n",
    "            rnd.shuffle(training_set)\n",
    "            mini_batches = [training_set[k*mini_batches_size:(k+1)*mini_batches_size] for k in range(n_mini_batches)]\n",
    "\n",
    "            for mini_batch in mini_batches:\n",
    "                zs = []\n",
    "                activations = []\n",
    "                errors = []\n",
    "\n",
    "\n",
    "\n",
    "                for training_input in mini_batch:\n",
    "                    #feedforward\n",
    "                    x = training_input[0]\n",
    "                    t = [training_input[1]]\n",
    "                    zs_x, activations_x = self._feedforward(x, training=True)\n",
    "                    zs.append(zs_x)\n",
    "                    activations.append(activations_x)\n",
    "\n",
    "                    #backprop of the error\n",
    "                    errors_x = self._backprop(zs_x, activations_x, t)\n",
    "                    errors.append(errors_x)\n",
    "\n",
    "                    #tracking\n",
    "                    cost += self._cost(activations_x[-1], t)\n",
    "                    #print(\"output: \", activations_x[-1])\n",
    "                    prediction = self.predict_highest(activations_x[-1], t)\n",
    "                    if prediction:\n",
    "                        accuracy += 1\n",
    "                    #tracking \n",
    "\n",
    "                for l in range(self.n_layers):\n",
    "                    errors_act_mean = np.zeros(self.weights[l].shape)\n",
    "                    errors_mean = np.zeros(self.biases[l].shape)\n",
    "\n",
    "                    #print(\"errors_mean shape of layer {0}: {1}\".format(l, errors_mean.shape))\n",
    "                    # Computing the mean of all the terms errors*activation (or delta*a)\n",
    "                    for error_x, activation_x in zip(errors, activations):\n",
    "                        #print(\"error_x of layer {0}: \\n{1}\".format(l, error_x[l]))\n",
    "                        #print(\"activations of layer {}: {}\".format(l, activations_x[l]))\n",
    "                        errors_act_mean += np.dot(error_x[l], activations_x[l].reshape(1, -1))\n",
    "                        error_x[l] = error_x[l].reshape(-1,)\n",
    "                        #print(\"error_x after reshaping\", error_x[l])\n",
    "                        errors_mean += error_x[l]\n",
    "                    errors_act_mean = np.true_divide(errors_act_mean, mini_batches_size)\n",
    "                    errors_mean = np.true_divide(errors_mean, mini_batches_size)\n",
    "\n",
    "                    self.weights[l] -= eta * errors_act_mean\n",
    "                    self.biases[l] -= eta * errors_mean\n",
    "\n",
    "            #tracking\n",
    "            cost = np.true_divide(cost, n_mini_batches)\n",
    "            accuracy /= (n_mini_batches*mini_batches_size)\n",
    "            print(\"\\nepochs {}\".format(i))\n",
    "            print(\"Cost = {}\".format(cost))\n",
    "            print(\"Accuracy = {:.2f}%\".format(accuracy*100))\n",
    "            print(\"progression = {:.2f}%\".format((100*(i+1)/epochs)))\n",
    "\n",
    "\n",
    "\n",
    "    def _backprop(self, zs, activations, t):\n",
    "        l = self.n_layers\n",
    "\n",
    "        errors = []\n",
    "        for layer in range(l):\n",
    "            errors.append([])\n",
    "\n",
    "        errors[-1] = self._cost_derivative(activations[-1], t) * self._activation_derivative(zs[-1], -1)\n",
    "\n",
    "        for i in range(2, l+1):\n",
    "            errors[l - i] = np.dot(self.weights[l+1 - i].transpose(), errors[l+1 - i]) * self._activation_derivative(zs[l - i], l-i)\n",
    "\n",
    "\n",
    "        for layer in range(l):\n",
    "            errors[layer] = errors[layer].reshape(-1, 1)\n",
    "\n",
    "        return errors\n",
    "\n",
    "\n",
    "    def summary(self):\n",
    "        summary = \"##################################\\n\"\n",
    "        summary += \"MODEL SUMMARY\\n\"\n",
    "        summary += \"Number of layers: {}\\n\".format(self.n_layers)\n",
    "        summary  += \"//////////////////////////////////\\n\"     \n",
    "        for layer in self.layers_info:\n",
    "            summary += \"layer {}: \".format(layer[\"index\"])\n",
    "            summary += \"Size: {} \".format(layer[\"size\"])\n",
    "            summary += \"Activation: {}\".format(layer[\"activation\"])\n",
    "            summary += \"\\n-------------------------------\\n\"\n",
    "        summary += \"End of summary\\n\"\n",
    "        summary += \"##################################\\n\"\n",
    "        #print(summary)\n",
    "        return(summary)\n",
    "\n",
    "    def create_training_set(self, inputs, targets):\n",
    "\n",
    "        training_set = []\n",
    "        for x, t in zip(inputs, targets):\n",
    "            training_set.append((x, t))\n",
    "\n",
    "        return training_set\n",
    "\n",
    "\n",
    "    def _feedforward(self, activation, training):\n",
    "        # Flatten the input\n",
    "        activation = self._flatten(activation)\n",
    "\n",
    "\n",
    "\n",
    "        #If we call this function to train the network,\n",
    "        # we need to record the pre-activations and activations\n",
    "        if training:\n",
    "            activations = [activation]\n",
    "            zs = []\n",
    "\n",
    "        for l in range(self.n_layers):\n",
    "\n",
    "            # Compute pre_activation\n",
    "            z = np.dot(self.weights[l], activation) + self.biases[l]\n",
    "            # Compute activation\n",
    "            activation = self._activation(z, l)\n",
    "\n",
    "            if training:\n",
    "                zs.append(z)\n",
    "                activations.append(activation)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if training:\n",
    "            #print(\"activations:\\n\", activations)\n",
    "            return zs, activations\n",
    "        else:\n",
    "            return activation\n",
    "\n",
    "\n",
    "    def _activation(self, z, layer):\n",
    "        act_fct = self.layers_info[layer][\"activation\"]\n",
    "        if act_fct == \"sigmoid\":\n",
    "            activation = self._sigmoid(z)\n",
    "        elif act_fct == \"relu\":\n",
    "            activation = self._relu(z)\n",
    "        else:\n",
    "            print(\"Error with the activation function of the layer number {}\".format(layer))\n",
    "\n",
    "        return activation\n",
    "\n",
    "    def _activation_derivative(self, z, layer):\n",
    "        act_fct = self.layers_info[layer][\"activation\"]\n",
    "        if act_fct == \"sigmoid\":\n",
    "            activation_prime = self._sigmoid_derivative(z)\n",
    "        elif act_fct == \"relu\":\n",
    "            activation_prime = self._relu_derivative(z)\n",
    "        else:\n",
    "            print(\"Error with the activation function of the layer number {}\".format(layer))\n",
    "\n",
    "        return activation_prime\n",
    "\n",
    "    def _create_layer(self, shape):\n",
    "        self.weights.append(np.random.randn(*shape))\n",
    "        self.biases.append(np.zeros(shape[0]))\n",
    "        #self.biases[-1] = self.biases[-1].reshape(-1, 1)\n",
    "\n",
    "\n",
    "    def _relu(self, z):\n",
    "        shape = z.shape\n",
    "        z = self._flatten(z)\n",
    "        new_z = []\n",
    "        for element in z:\n",
    "            if element <= 0:\n",
    "                new_z.append(0)\n",
    "            else:\n",
    "                new_z.append(element)\n",
    "        new_z = np.array(new_z)\n",
    "        return new_z.reshape(shape)\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        return 1 / (1 + np.e ** (-z))\n",
    "\n",
    "\n",
    "    def _flatten(self, vector):\n",
    "        shape = vector.shape\n",
    "        size = 1\n",
    "        for dimension in shape:\n",
    "            size *= dimension\n",
    "\n",
    "        return vector.reshape(size)\n",
    "\n",
    "    def _cost(self, y, t):\n",
    "        return 0.5*(y - t)**2\n",
    "\n",
    "    def _cost_derivative(self, y, t):\n",
    "        return (y - t)\n",
    "\n",
    "    def _sigmoid_derivative(self, z):\n",
    "        return self._sigmoid(z) * (1 - self._sigmoid(z))\n",
    "\n",
    "    def _relu_derivative(self, z):\n",
    "        shape = z.shape\n",
    "        z = self._flatten(z)\n",
    "        new_z = []\n",
    "        for element in z:\n",
    "            if element <= 0:\n",
    "                new_z.append(0)\n",
    "            else:\n",
    "                new_z.append(1)\n",
    "        new_z = np.array(new_z)\n",
    "        return new_z.reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(row_per_class):\n",
    "    \"\"\"\n",
    "    Method used to get the dataset\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    #generate rows\n",
    "\n",
    "    sick = np.random.randn(row_per_class, 2) + np.array([-2, 2])\n",
    "    sick2 = np.random.randn(row_per_class, 2) + np.array([2, -2])\n",
    "\n",
    "\n",
    "    healthy = np.random.randn(row_per_class, 2) + np.array([2, 2])\n",
    "    healthy2 = np.random.randn(row_per_class, 2) + np.array([-2, -2])\n",
    "\n",
    "    features = np.vstack([sick, sick2, healthy, healthy2])\n",
    "    targets = np.concatenate((np.zeros(row_per_class*2) + 1, np.zeros(row_per_class*2)))\n",
    "    return features, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, targets = get_dataset(10)\n",
    "plt.scatter(features[0:21].transpose()[0], features[0:21].transpose()[1], c=\"blue\")\n",
    "plt.scatter(features[21:].transpose()[0], features[21:].transpose()[1], c=\"red\")\n",
    "plt.show()\n",
    "print(features.shape)\n",
    "print(features[0:4])\n",
    "print(targets[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_layer(10, \"relu\")\n",
    "model.add_layer(5, \"relu\")\n",
    "model.add_layer(1, \"sigmoid\")\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.initiate_layers([2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model._feedforward(features[4], targets[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit(features, targets, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"features: \", features)\n",
    "print(\"targets: \", targets)\n",
    "for feature in features:\n",
    "    prediction = model._feedforward(feature, False)\n",
    "    print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
